<h3 align="center">Customize-arXiv-Daily</h3>

---

<p align="center"> Recommend new arxiv papers of your interest daily according to your customized description.
    <br> 
</p>

> [!NOTE]
> This repo borrow the idea and some functions from [zotero-arxiv-daily](https://github.com/TideDra/zotero-arxiv-daily). Thanks for their great work!ğŸ˜Š

## ğŸ§ Why I create this project <a name = "about"></a>

- During the use of [zotero-arxiv-daily](https://github.com/TideDra/zotero-arxiv-daily), I often find that the recommendation process didn't run in the way that I want. Since my study area has shifted, my Zotero include some papers that I'm not interested in anymore.
- For those who **do not use zotero as PDF reader**, get customized arxiv recommendation is still needed.
- For those that want to **set their own prompt** to guide LLM during paper selection and recommendation.

## âœ¨ Key Features Compared with [zotero-arxiv-daily](https://github.com/TideDra/zotero-arxiv-daily)

- Fully customized LLM prompt to guide your paper recommendation process.
- Ready-to-use leverage of recent models, include DeepSeek-R1/V3/...
- Save your arXiv recommendation history.
- Batch LLM scoring + rerank Top-M to reduce ties and stabilize ranking.
- Fixed Top-5 recommendations at the start of the email.
- Support multiple workers to speed up the recommendation process.

## ğŸ“· Screenshot

![screenshot](./assets/screenshot.png)

## ğŸš€ Usage

### Quick Start

1. Run `git clone https://github.com/JoeLeelyf/customize-arxiv-daily.git`
2. Run `pip install -r requirements.txt` to install necessary packages.
   If you prefer using [`uv`](https://github.com/astral-sh/uv) for dependency management, run:

   ```bash
   uv sync
   ```

   This will create the virtual environment described by `pyproject.toml` and `uv.lock`.
3. Get your STMP server. Common STMP service provider includes [QQ mail box](https://service.mail.qq.com/detail/0/427)
4. Describe the research fields you're interested in, and the fields you're not. Edit the `description.txt`. For, example:

```txt
I am working on the research area of computer vision.
Specifically, I am interested in the following fieds:
1. Object detection
2. AIGC (AI Generated Content)
3. Multimodal Large Language Models

I'm not interested in the following fields:
1. 3D Vision
2. Robotics
3. Low-level Vision
```

5. Configure your own `arXiv catergories`, `api_key` and `models`. The repo supports **OpenAI-compatible** Chat Completions endpoints (including third-party services that expose the OpenAI API format). Meaning of different parameters:
   - `--categories`: arXiv categories that you are interested in, like `cs.CV` `cs.AI`
   - `--sender`: E-mail address that provide SMTP service, like, `123456@qq.com`
   - `--receiver`: The e-mails address that you want to receive your notice, like, `my_gmail@gmail.com`
   - `--save`: store_true, whether to save the arXiv results to local markdown files.

- `main_gpt.sh`: Example runner for OpenAI-compatible APIs (supports model failover list and endpoint failover list).

```bash
python main.py --categories cs.CV cs.AI \
    --model gpt-4o-mini \
    --base_url https://api.openai.com/v1 --api_key * \
    --smtp_server smtp.qq.com --smtp_port 465 \
    --sender * --receiver * \
    --sender_password * \
    --num_workers 16 \
    --lookback_hours 24 \
    --llm_batch_size 5 \
    --rerank_top_m 30 \
    --title "Daily arXiv" \
    --temperature 0.7 \
    --save
```

6. Choose to run one of the following command in your CLI.

```
bash main_gpt.sh
```


### Run with uv

After syncing dependencies you can execute the CLI through `uv run` (it will reuse the managed environment):

```bash
uv run python main.py --categories cs.CV cs.AI \
    --model gpt-4o-mini \
    --base_url https://api.openai.com/v1 --api_key * \
    --smtp_server smtp.qq.com --smtp_port 465 \
    --sender * --receiver * \
    --sender_password * \
    --num_workers 16 \
    --lookback_hours 24 \
    --llm_batch_size 5 \
    --rerank_top_m 30 \
    --title "Daily arXiv" \
    --temperature 0.7 \
    --save
```

7. \* **Run automatically everyday (GitHub Actions recommended).**

This repo is designed to run daily with a **longer window** (e.g. last 4 days) + a persistent `seen_ids` database to avoid re-processing/re-emailing papers:

- Window: `--lookback_hours 96` (covers weekend backlog)
- Seen DB: `--seen_db state/seen_ids.json --seen_retention_days 30 --seen_scope base`

Create GitHub Secrets:
- `MODELSCOPE_API_KEY` (or your OpenAI-compatible API key)
- `SMTP_SENDER`, `SMTP_RECEIVER`, `SMTP_PASSWORD`

Then enable the workflow file: `.github/workflows/daily.yml` (it runs `bash main_gpt.sh` and commits the updated `state/seen_ids.json` back to the repo).

8. \* **Adjust and customize your LLM prompt.** Edit `_build_batch_prompt(...)` / `_build_rerank_prompt(...)` in `arxiv_daily.py`.

## Results

### Running process in your CLI

![CLI](./assets/cli.png)

### Markdown saved

![Markdown](./assets/markdown.png)

### E-mail received

![Screenshot](./assets/screenshot.png)

## ğŸ“– How it works

- `util/request.py` fetches recent arXiv papers given your provided arXiv categories (via arXiv Atom API), and keeps only papers within the last 24 hours by default.
- `arxiv_daily` will call LLM api to summarize every paper and get the relevance score.
- `util/construct_email.py` construct the content of the email in HTML form and send it using SMTP service.

## ğŸ”§ ä¸â€œè·å–è®ºæ–‡/ç­›é€‰è®ºæ–‡â€å¯†åˆ‡ç›¸å…³çš„è¶…å‚æ•°

ä¸‹é¢è¿™äº›å‚æ•°ä¼šç›´æ¥å½±å“**æŠ“å–åˆ°çš„å€™é€‰è®ºæ–‡åˆ—è¡¨**ä»¥åŠ **LLM çš„ç­›é€‰/æ’åºç»“æœ**ï¼ˆä¹Ÿä¼šå½±å“è¿è¡Œæ—¶é—´ä¸æˆæœ¬ï¼‰ï¼š

### è·å–è®ºæ–‡åˆ—è¡¨ï¼ˆæŠ“å–ä¾§ï¼‰

- `--categories`ï¼šè¦æŠ“å–çš„ arXiv åˆ†ç±»ï¼ˆä¾‹å¦‚ `cs.CV cs.AI`ï¼‰ã€‚è„šæœ¬ä¼šå¯¹æ¯ä¸ªåˆ†ç±»æ‹‰å–æœ€è¿‘æ¡ç›®ï¼Œç„¶ååˆå¹¶å»é‡ã€‚
- `--max_entries`ï¼šæ¯ä¸ªåˆ†ç±»æ‹‰å–çš„æœ€å¤§æ¡ç›®æ•°ï¼ˆæŒ‰æäº¤æ—¶é—´å€’åºï¼‰ã€‚è¶Šå¤§è¡¨ç¤ºå€™é€‰è¶Šå¤šã€LLM è°ƒç”¨è¶Šå¤šã€‚
- `--lookback_hours`ï¼šä»…ä¿ç•™è¿‡å» N å°æ—¶å†…çš„æ–°è®ºæ–‡ï¼ˆé»˜è®¤ `24`ï¼‰ã€‚å»ºè®®é…åˆâ€œæ¯å¤©è¿è¡Œ 1 æ¬¡â€ä½¿ç”¨ï¼Œé¿å…é‡å¤ç­›åŒä¸€æ‰¹è®ºæ–‡ã€‚
- `--description`ï¼šç ”ç©¶å…´è¶£æè¿°æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ `description.txt`ï¼‰ã€‚è¯¥å†…å®¹ä¼šè¿›å…¥æç¤ºè¯ï¼Œç›´æ¥å½±å“ LLM ç›¸å…³æ€§åˆ¤æ–­ã€‚
- `--include_keywords`ï¼šå…³é”®è¯åŒ…å«è¿‡æ»¤ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰ã€‚ç”¨äºåœ¨è¿›å…¥ LLM å‰å…ˆç¼©å°å€™é€‰é›†ã€‚
- `--exclude_keywords`ï¼šå…³é”®è¯æ’é™¤è¿‡æ»¤ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰ã€‚å‘½ä¸­ä»»ä¸€å…³é”®è¯åˆ™å‰”é™¤ã€‚
- `--include_mode`ï¼š`include_keywords` çš„å‘½ä¸­è§„åˆ™ï¼š`any`ï¼ˆå‘½ä¸­ä»»ä¸€ï¼‰/ `all`ï¼ˆå¿…é¡»å‘½ä¸­å…¨éƒ¨ï¼‰ã€‚

### ç­›é€‰/æ’åºï¼ˆLLM ä¾§ï¼‰

- `--max_paper_num`ï¼šæœ€ç»ˆä¿ç•™å¹¶è¾“å‡ºçš„ Top-N è®ºæ–‡æ•°ï¼ˆæŒ‰ `relevance_score` é™åºæˆªæ–­ï¼‰ã€‚æ³¨æ„ï¼š**è¿™ä¸ä¼šå‡å°‘ LLM è°ƒç”¨æ¬¡æ•°**ï¼Œå®ƒåªå†³å®šæœ€ç»ˆè¾“å‡ºæ•°é‡ã€‚
- `--llm_batch_size`ï¼šLLM æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ `5`ï¼‰ã€‚æ¯æ¬¡è°ƒç”¨ä¼šåŒæ—¶å¤„ç† N ç¯‡è®ºæ–‡ï¼Œå¯å‡å°‘è¯·æ±‚æ¬¡æ•°å¹¶æå‡è¯„åˆ†ç¨³å®šæ€§ã€‚
- `--num_workers`ï¼šå¹¶å‘ worker æ•°ï¼ˆçº¿ç¨‹æ± ï¼‰ã€‚è¶Šå¤§è¶Šå¿«ï¼Œä½†æ›´å®¹æ˜“è§¦å‘ API é™æµ/æœ¬åœ°æ¨¡å‹èµ„æºä¸è¶³ã€‚
- `--temperature`ï¼šLLM é‡‡æ ·æ¸©åº¦ã€‚è¶Šé«˜è¾“å‡ºè¶Šâ€œå‘æ•£â€ï¼Œç›¸å…³æ€§è¯„åˆ†ä¸æ‘˜è¦ç¨³å®šæ€§è¶Šå·®ï¼›è¶Šä½æ›´ç¨³å®šä½†å¯èƒ½æ›´â€œä¿å®ˆâ€ã€‚
- `--weight_topic/--weight_method/--weight_novelty/--weight_impact`ï¼šå¤šç»´åº¦è¯„åˆ†çš„åŠ æƒç³»æ•°ï¼ˆæ€»åˆ†ç”±å››é¡¹åŠ æƒå¾—åˆ°ï¼Œé»˜è®¤ `0.45/0.25/0.15/0.15`ï¼‰ã€‚
- `--rerank_top_m`ï¼šæœ€ç»ˆå¯¹ Top-M å€™é€‰åšä¸€æ¬¡â€œå…¨å±€æ¯”è¾ƒå¼é‡æ’â€ï¼ˆé»˜è®¤ `30`ï¼Œè¾“å…¥ä¸º title+abstractï¼‰ã€‚ç”¨äºå‡å°‘åŒåˆ†ä¸çº åï¼›è®¾ä¸º `0` å¯å…³é—­ã€‚
- `--base_url/--api_key/--model`ï¼šæ”¯æŒä¼ å…¥å¤šä¸ªå€¼ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ã€‚å½“ä¸€æ¬¡è¯·æ±‚æŠ¥é”™æ—¶ä¼šæŒ‰åˆ—è¡¨é¡ºåºè‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªï¼ˆå¯ç»„æˆ base_url+api_key+model çš„ä¸‰å…ƒç»„åˆ—è¡¨ï¼›å½“ model ä¸ºåˆ—è¡¨æ—¶ï¼Œä¼šä¼˜å…ˆæŒ‰ä¸‰å…ƒç»„é¡ºåºåˆ‡æ¢ï¼‰ã€‚
- `--seen_db/--seen_retention_days/--seen_scope`ï¼šé•¿çª—å£æ¨¡å¼ä¸‹çš„â€œå·²å¤„ç†è®ºæ–‡ IDâ€å»é‡æœºåˆ¶ã€‚æ¨è `--lookback_hours 96` + `--seen_retention_days 30` è¦†ç›–å‘¨æœ«å †ç§¯ï¼ŒåŒæ—¶é¿å…é‡å¤å¤„ç†/é‡å¤å‘é‚®ä»¶ã€‚

### è¿è¡Œæœºåˆ¶è¡¥å……ï¼ˆä¾¿äºç†è§£ä¸Šè¿°å‚æ•°çš„å½±å“ï¼‰

- **åˆå¹¶å»é‡**ï¼šå¤šåˆ†ç±»æŠ“å–ç»“æœä¼šæŒ‰ `arXiv_id` å»é‡åå†è¿›å…¥ LLM é˜¶æ®µã€‚
- **æ¯æ—¥å›ºå®šæ¨è**ï¼šé‚®ä»¶å¼€å¤´å›ºå®šå±•ç¤ºè¯„åˆ†æœ€é«˜çš„å‰ 5 ç¯‡è®ºæ–‡ï¼ˆé™åºï¼‰ã€‚
- **ç¼“å­˜ï¼ˆå¼€å¯ `--save` æ—¶ï¼‰**ï¼šæ¯ç¯‡è®ºæ–‡çš„ LLM ç»“æœä¼šç¼“å­˜åˆ° `arxiv_history/<date>/json/<arXiv_id>.json`ï¼Œé‡å¤è¿è¡ŒåŒä¸€å¤©é€šå¸¸ä¼šå¤ç”¨ç¼“å­˜ï¼Œæ˜¾è‘—å‡å°‘ LLM è°ƒç”¨ã€‚

## ğŸ“Œ Limitations

- The recommendation process of LLM is unstable and the relevance score provided by different LLMs varies a lot.
